# 线性注意力的双重视角：损失函数演化与线性化方法

## 核心问题

Transformer 的 Softmax Attention 计算复杂度为 O(n²)，这在长序列场景下成为瓶颈。线性注意力试图将其降低到 O(n)，但如何在保持性能的同时实现线性化？

有两条完全不同的路径：

1. **近似路径**：用核方法近似 softmax（Performer）
2. **替代路径**：用新的损失函数替代 softmax（DeltaNet/KDA）

本文将系统梳理这两条路径的数学原理和演化逻辑。

---

## 工业应用视角：为什么线性注意力现在才被重视？

### 关键洞察

**Linear Attention 以前没人用，是因为以前真的很菜；现在开始有人用，是因为第二代变体变强了，且混合架构成了主流。**

这不仅仅是工程精细化，而是发生了两次本质的**版本迭代**。

### Generation 1（2020-2022）：学术界热闹，工业界弃用

**代表作**：Linear Transformer, Performer, Linformer

**状态**：除了极个别实验性项目，主流大模型（GPT-3, PaLM, Llama 1/2）完全不碰 Linear Attention。

#### 为什么没人用？

**A. "召回灾难"（The Recall Collapse）**

这是 Linear Attention 的致命伤：

- **Softmax Attention**：擅长"大海捞针"。如果某个 Key 和 Query 极其匹配，Softmax 会给它 0.99 的权重，其他给 0.0001。这是**赢家通吃（Winner-takes-all）**机制。

- **Old Linear Attention (Performer)**：由于是加权平均，很难产生"尖锐"的分布，倾向于把权重"平摊"给所有 token。

- **后果**：**联想记忆（Associative Recall）**能力极差。

示例：
```
Prompt: "Alice 的电话号码是 12345... [中间隔了10k字] ... Alice 的电话是多少？"
Softmax:    "12345" （精准定位）
Performer:  "..." （大概率胡说八道，或关注到错误重点）
```

**B. 训练速度并没有变快**

虽然推理是 O(n)，但训练时：
- 为了并行化需要 Parallel Scan / Prefix Sum
- 早期 CUDA kernel 优化很差
- 实际跑起来比高度优化的 FlashAttention 还慢

**结论**：效果差 + 训练没优势 = 无人问津

### Generation 2（2023-2025）：开始挑战 Transformer

**代表作**：RWKV, RetNet, Mamba (SSM), Kimi Linear (KDA)

**状态**：开始挑战 Transformer 统治地位，特别是在长文本领域。

#### 为什么现在变火了？

**A. 不再模仿 Softmax，学会了"遗忘"**

这一代模型引入了：
- **指数衰减（Decay）**：让信息随时间衰减
- **数据依赖门控（Gating）**：主动选择忽略不重要信息
- **Delta Rule**：更强的记忆更新机制

**结果**：
- 虽然在极端的"大海捞针"上可能略逊于 Softmax
- 但在一般长文本理解、代码补全上，性能已几乎持平（Parity）

**B. 显存墙（The Memory Wall）与 1M Context**

这是最现实的工程推手：

| 模型类型 | 推理显存占用 | 1M context 代价 |
|---------|------------|----------------|
| Transformer | KV Cache: O(n) | 撑爆 8 张 H100 |
| Linear Attention | 状态矩阵: O(1) | 几百 MB（固定） |

**商业驱动**：
- Kimi、Claude 主打"超长无损上下文"
- 如果用标准 Attention，推理成本太高
- Linear Attention 是降低推理成本的**唯一数学解**

### 现在的真实用法：混合架构才是王道

**工业界的版本答案**：不要全用 Linear，也不要全用 Softmax。

#### 混合架构实例

| 模型 | 架构组合 | 设计思路 |
|------|---------|---------|
| **Kimi Linear** | 大部分 KDA + 1-2 层 MLA | 95% 用 Linear 理解，5% 用 Softmax 精准检索 |
| **Google Griffin/Hawk** | RG-LRU + Local Attention | Linear 处理全局，Attention 处理局部 |
| **Jamba (AI21)** | Mamba 层 + Transformer 层 | 交替使用，优势互补 |

#### 为什么混合？

- **Linear 层**（95% 工作量）：
  - 处理语法、语义、上下文流
  - 享受 O(1) 推理显存优势
  - 高效处理海量背景信息

- **Softmax 层**（5% 工作量）：
  - 精准查阅（人名、电话号码、特定事实）
  - 解决"召回灾难"
  - 处理需要精确匹配的场景

### 汇报要点总结

如果你要向他人解释"为什么 Linear Attention 现在值得关注"：

1. **机制变了**：从拙劣的 Softmax 模仿者（Performer），变成基于门控和 Delta Rule 的高效学习者（RetNet/KDA）。

2. **目标变了**：不是为了替代 Transformer，而是为了"超长上下文"。在 4k 长度下没人在意，但在 128k 或 1M 长度下，它是必经之路。

3. **策略变了**：工业界不再争论"Linear vs Softmax"，而是转向"Linear + Softmax"混合架构，取长补短。

**一句话结论**：

> Linear Attention 以前是"为了线性而线性的数学玩具"，效果很差；现在是"为了解决 1M 上下文显存瓶颈的特种武器"，且通过混合架构解决了效果问题。

---

---

## 第一部分：Softmax Attention 的线性化

### 1.1 标准 Softmax Attention

给定查询 q 和键值对 {k_j, v_j}，标准的 attention 输出为：

```
y(q) = Σ_j softmax(q^T k_j) v_j
     = Σ_j [exp(q^T k_j) / Σ_i exp(q^T k_i)] v_j
```

分子分母分别为：

```
分子：Σ_j exp(q^T k_j) v_j
分母：Σ_j exp(q^T k_j)
```

**关键问题**：这个形式无法写成 q 和 k 的独立函数乘积形式，因此无法避免 O(n²) 的成对计算。

### 1.2 线性化的关键思想

如果能把 exp(q^T k) 写成：

```
exp(q^T k) ≈ φ(q)^T φ(k)
```

那么：

```
分子 = Σ_j φ(q)^T φ(k_j) v_j 
     = φ(q)^T (Σ_j φ(k_j) v_j^T)
     = φ(q)^T S

分母 = Σ_j φ(q)^T φ(k_j)
     = φ(q)^T (Σ_j φ(k_j))
     = φ(q)^T z
```

其中：
- S = Σ_j φ(k_j) v_j^T  （可以增量维护）
- z = Σ_j φ(k_j)         （可以增量维护）

这样就实现了 O(n) 复杂度！

### 1.3 Performer：用核方法实现线性化

**目标**：构造 φ 使得 exp(q^T k) ≈ E[φ_ω(q) φ_ω(k)]

#### 步骤 1：利用二次型恒等式

```
q^T k = -1/2 |q-k|²+ 1/2 |q|²+ 1/2 |k|²
```

因此：

```
exp(q^T k) = exp(-1/2 |q|²) exp(-1/2 |k|²) exp(-1/2 |q-k|²)
```

#### 步骤 2：高斯核的母函数性质

对于标准高斯随机向量 ω ~ N(0, I)：

```
E[exp(ω^T u)] = exp(1/2 |u|²)
```

因此：

```
exp(-1/2 |q-k|²) = E[exp(-ω^T (q-k))]
                  = E[exp(-ω^T q) exp(ω^T k)]
```

#### 步骤 3：得到最终形式

```
exp(q^T k) = exp(-1/2 |q|²) exp(-1/2 |k|²) E[exp(-ω^T q) exp(ω^T k)]
           = E[φ_ω(q) φ_ω(k)]
```

其中：

```
φ_ω(x) = exp(ω^T x - 1/2 |x|²)
```

#### 步骤 4：Monte Carlo 近似

用 r 个随机向量 {ω_1, ..., ω_r} 近似期望：

```
φ(x) = 1/√r [φ_ω1(x), φ_ω2(x), ..., φ_ωr(x)]
```

**复杂度分析**：
- 原始：O(n² d)
- Performer：O(n d r)，通常 r << n

### 1.4 小结：Performer 的本质

Performer 是一个**近似方法**：
- 它试图保留 softmax 的语义
- 用随机特征近似核函数
- 理论上可以任意精确（增大 r）
- 但实践中 r 受限于计算资源

---

## 第二部分：从损失函数视角看线性注意力的演化

这是一条完全不同的路径：**不近似 softmax，而是直接替代它**。

### 2.1 传统 Linear Attention（2020）

#### 最简单的线性形式

```
y_t = S_t q_t / (z_t^T q_t)

其中：
S_t = S_{t-1} + k_t v_t^T
z_t = z_{t-1} + k_t
```

这等价于什么损失函数？

#### 隐含的最大化目标

从 fast-weight 视角，这相当于最大化：

```
L = q^T S v = q^T (Σ k_j v_j^T) v
```

**问题**：
1. 这个目标是**无界的**（S 不断累加）
2. 没有归一化约束
3. 无法遗忘旧信息
4. 长序列会发散

### 2.2 DeltaNet（2023）：引入重构损失

#### 核心创新：最小化重构误差

DeltaNet 提出一个完全不同的目标——不是最大化相关性，而是最小化预测误差：

```
L_t(S) = 1/2 |S^T k_t - v_t|²
```

这个损失函数说的是：**用 S 从 k_t 重构 v_t**。

#### 梯度下降更新

对 S 求梯度：

```
∇_S L = (S^T k_t - v_t) k_t^T
```

梯度下降更新（学习率 β_t）：

```
S_t = S_{t-1} - β_t ∇_S L
    = S_{t-1} - β_t (S_{t-1}^T k_t - v_t) k_t^T
    = S_{t-1} - β_t k_t k_t^T S_{t-1} + β_t k_t v_t^T
    = (I - β_t k_t k_t^T) S_{t-1} + β_t k_t v_t^T
```

#### 关键特性

与传统 Linear Attention 对比：

| 特性 | Linear Attention | DeltaNet |
|------|-----------------|----------|
| 更新形式 | S + k v^T | (I - β k k^T) S + β k v^T |
| 是否可减 | ✗ 只增不减 | ✓ 可增可减 |
| 是否有界 | ✗ 无界增长 | ✓ 有隐式正则化 |
| 误差修正 | ✗ 无法修正 | ✓ self-correcting |

**数学直觉**：
- 传统 LA：累加所有信息（无选择）
- DeltaNet：根据重构误差调整记忆（有选择）

### 2.3 Delta Rule 的历史渊源

DeltaNet 的更新规则不是凭空发明的，而是经典 **Delta Rule** 的矩阵扩展。

#### Widrow-Hoff 规则（1960）

单个神经元的学习规则：

```
Δw = η (y - ŷ) x
```

其中：
- y：真实值
- ŷ = w^T x：预测值
- η：学习率

这就是 **Delta Rule**（也叫 LMS 规则）。

#### DeltaNet 是 Delta Rule 的矩阵版本

```
单神经元：Δw = η (y - w^T x) x
矩阵版本：ΔS = β (v - S^T k) k^T
```

展开就得到 DeltaNet 的更新公式。

#### 相关工作

Delta Rule 的思想贯穿多个领域：

1. **信号处理**：LMS、RLS 算法
2. **神经科学**：Rescorla-Wagner 模型
3. **神经网络**：反向传播的祖先
4. **联想记忆**：Hopfield Network（1982）
5. **现代版本**：Modern Hopfield（2020）

DeltaNet 将这个 60 年的经典思想引入了 Transformer。

### 2.4 Gated DeltaNet（2024）：引入标量遗忘门

虽然 DeltaNet 能自我修正，但它仍然缺乏**显式的遗忘机制**。

#### 添加全局衰减

```
S_t = α_t (I - β_t k_t k_t^T) S_{t-1} + β_t k_t v_t^T
```

其中 α_t ∈ (0,1] 是一个标量，控制整个状态矩阵的衰减速度。

**效果**：
- α_t ≈ 1：长期记忆
- α_t ≈ 0：短期记忆

**问题**：
- 所有维度一刀切
- 无法实现细粒度控制
- 比如：某些维度需要长期记忆，某些需要快速遗忘

### 2.5 Kimi Delta Attention（2025）：对角门控

#### 核心创新：标量门 → 对角门

```
S_t = (I - β_t k_t k_t^T) Diag(α_t) S_{t-1} + β_t k_t v_t^T
```

其中 α_t ∈ R^d 是一个**向量**，每个维度有独立的遗忘率。

#### 为什么对角门如此重要？

**表达能力**：

| 门控类型 | 参数量 | 表达能力 | 计算复杂度 |
|---------|--------|---------|-----------|
| 无门控 | 0 | 最弱 | O(d²) |
| 标量门 | 1 | 弱 | O(d²) |
| 对角门 | d | 强 | O(d²) |
| 全矩阵门 | d² | 最强 | O(d³) |

对角门是**最优的折中**：
- 复杂度仍为 O(d²)
- 但表达能力接近全矩阵门
- 每个维度独立控制记忆寿命

#### 工程优势：保持 Rank-1 结构

关键观察：

```
(I - β k k^T) Diag(α) S
```

虽然中间有 Diag(α)，但整体仍是 **Diagonal + Rank-1**（DPLR）结构。

这意味着：
- 可以 chunk-wise 并行
- 可以用 WY representation
- 可以高效实现

### 2.6 损失函数视角的演化链条

| 阶段 | 损失函数 | 更新规则 | 遗忘机制 |
|------|---------|---------|---------|
| Linear Attention | 最大化 q^T S v | S + k v^T | 无 |
| DeltaNet | 最小化 \|S^T k - v\|² | (I - βkk^T)S + βkv^T | 隐式 |
| Gated DeltaNet | 同上 + L2正则 | α(I - βkk^T)S + βkv^T | 全局标量 |
| KDA | 同上 + 维度正则 | (I - βkk^T)Diag(α)S + βkv^T | 细粒度对角 |

**演化逻辑**：
```
无界累加 → 有界重构 → 全局遗忘 → 细粒度遗忘
```

---

## 第三部分：KDA 的 Chunk-based 并行化

### 3.1 为什么需要 Chunk-based？

虽然 DeltaNet/KDA 是 O(n) 复杂度，但它本质是一个 **RNN**：

```
S_t = A_t S_{t-1} + B_t
```

问题：
- 必须按时间顺序计算
- 无法并行
- GPU 利用率低
- 长序列仍然慢

### 3.2 核心思想：合并多步更新

如果我们能把一个 chunk 内的多步更新合并：

```
S_t+C = A_{t+C} A_{t+C-1} ... A_{t+1} S_t + [累积的 B]
```

并且这个合并后的矩阵仍保持某种结构，就可以并行计算！

### 3.3 关键：DPLR 结构的封闭性

DeltaNet/KDA 的每步更新是：

```
A_t = (I - β_t k_t k_t^T) Diag(α_t)
    = Diagonal - Rank-1
```

这是 **DPLR (Diagonal Plus Low-Rank)** 结构。

**关键性质**：DPLR 矩阵的乘积仍是 DPLR！

```
(D_1 - U_1 V_1^T)(D_2 - U_2 V_2^T) = D_3 - U_3 V_3^T
```

其中 rank 可以保持在 O(r)（r 很小）。

### 3.4 WY Representation

这利用了 Householder 反射的性质。多个 rank-1 更新可以合并为：

```
(I - u_1 v_1^T)(I - u_2 v_2^T)...(I - u_r v_r^T) = I - U T V^T
```

其中 T 是 r×r 的上三角矩阵。

### 3.5 Chunk-based 算法流程

1. **分块**：将序列分成长度为 C 的 chunks
2. **合并**：每个 chunk 内的 A_t 合并为一个 DPLR 矩阵
3. **并行**：不同 chunks 之间的状态更新可以并行
4. **高效**：DPLR 矩阵乘法是 O(rd)，r << d

**复杂度**：
- 串行：O(n)
- Chunk-based：O(n/C) 次状态更新，每次 O(Cr²d)
- 实际：由于并行，wall-clock time 大幅减少

### 3.6 为什么 KDA 能做到，Mamba 不能？

Mamba 的状态更新不是 DPLR 结构，而是更复杂的形式，无法高效合并。

KDA 的 Delta Rule + Diagonal Gate 恰好保持了：
- Rank-1 更新
- 与 Householder 结构兼容
- 可以 WY 合并
- 保持低 rank

这是理论设计和工程实现的完美结合。

---

## 第四部分：两条路径的对比与统一

### 4.1 对比总结

| 维度 | Performer | DeltaNet/KDA |
|------|-----------|-------------|
| **目标** | 近似 softmax | 替代 softmax |
| **方法** | 核展开 + 随机特征 | Delta Rule + 门控 |
| **理论基础** | 概率论 + 核方法 | 优化理论 + 自适应滤波 |
| **是否保留 softmax 语义** | 是（近似） | 否（新损失） |
| **精度控制** | 增大 r | 改进门控 |
| **并行化** | 天然线性 | 需要 chunk-based |
| **历史渊源** | RKHS、随机特征 | Delta Rule、Hopfield |

### 4.2 统一视角

这两条路径都在解决同一个问题：**如何绕过 O(n²) 的成对计算**。

**Performer**：
```
exp(q^T k) → φ(q)^T φ(k)
```
通过引入中间表示 φ，将成对计算转化为对状态矩阵的线性操作。

**DeltaNet/KDA**：
```
softmax attention → recurrent update on S
```
通过引入状态矩阵 S，将全局依赖转化为局部递推。

### 4.3 未来方向

1. **混合方法**：能否结合核方法和 Delta Rule？
2. **更好的门控**：对角门是否已是最优？
3. **理论分析**：KDA 相比 softmax 损失了什么，又获得了什么？
4. **硬件协同**：如何针对 GPU/NPU 优化 chunk-based 算法？

---

## 总结

### 核心要点

1. **两条路径**：
   - 近似路径（Performer）：保持 softmax 语义
   - 替代路径（DeltaNet/KDA）：用新的损失函数

2. **损失函数的演化**：
   - 无界最大化 → 重构误差 → 带遗忘的重构

3. **门控的演化**：
   - 无门控 → 标量门 → 对角门

4. **工程突破**：
   - Chunk-based 并行化使得 RNN-style 更新也能高效

5. **理论根源**：
   - Performer：随机特征 + 核方法
   - DeltaNet：Delta Rule + 自适应滤波

### 关键洞察

**线性注意力不是一个方法，而是一类方法**。它们的共同点是实现 O(n) 复杂度，但：
- 手段不同（近似 vs 替代）
- 假设不同（softmax 必要 vs 可替代）
- 工程实现不同（天然并行 vs 需要 chunking）

KDA 代表了**替代路径的当前最优解**：理论上继承了 Delta Rule 的 60 年智慧，工程上通过 DPLR + chunking 实现了超越 full attention 的性能。

### 工业应用时间线

| 时期 | 技术代 | 代表模型 | 工业采用情况 | 核心问题/突破 |
|------|--------|---------|------------|-------------|
| 2020-2022 | Gen 1 | Performer, Linformer | ❌ 几乎无人采用 | 召回灾难 + 训练不快 |
| 2023-2024 | Gen 2 早期 | RWKV, RetNet | 🟡 实验性采用 | 引入门控和衰减 |
| 2024-2025 | Gen 2 成熟 | Mamba, KDA | ✅ 工业级部署 | 混合架构 + 长文本优势 |

### 架构演化路线图

```
2020: 纯 Softmax Transformer
         ↓
2021: 尝试纯 Linear（失败）
         ↓
2023: 引入门控机制（转机）
         ↓
2024: 混合架构（成功）
         ↓
2025: Linear 主导 + Softmax 辅助（主流）
```

### 不同场景的最佳选择

| 场景 | Context 长度 | 推荐架构 | 原因 |
|------|------------|---------|------|
| 对话助手 | < 4k | 纯 Softmax | 召回精度重要 |
| 代码补全 | 4k-32k | 混合架构 | 平衡性能与成本 |
| 文档分析 | 32k-128k | Linear 主导 | 显存瓶颈明显 |
| 超长上下文 | > 128k | Linear + 稀疏检索 | 唯一可行方案 |
| RL/Agent | 实时流 | 纯 Linear (RNN mode) | 需要 O(1) 状态 |

### 技术选型决策树

```
是否需要 > 32k context？
├─ 否 → 用标准 Transformer
│       （成熟、效果好、生态完善）
│
└─ 是 → 是否需要精确检索？
    ├─ 是 → 混合架构
    │       （95% Linear + 5% Softmax）
    │
    └─ 否 → 纯 Linear
            （最大化推理效率）
```

### 对从业者的启示

1. **不要盲目追求纯线性**：混合架构才是工业级解决方案

2. **关注真实瓶颈**：
   - 短文本：优化 FlashAttention
   - 长文本：必须考虑 Linear
   - 实时流：只能用 RNN-mode Linear

3. **理解数学本质**：
   - Performer：近似游戏（永远略逊一筹）
   - KDA：新游戏（不同的优化目标）

4. **工程与理论并重**：
   - 理论：Delta Rule 提供了正确的更新机制
   - 工程：DPLR + Chunking 让它真正可用

5. **关注混合比例**：
   - 不同任务需要不同的 Linear/Softmax 比例
   - 这是一个可调的超参数
   - 未来可能出现"动态混合"机制

### 未来趋势预测

1. **架构层面**：
   - 混合架构将成为标配
   - 动态路由（任务自适应选择 Linear/Softmax）
   - 更细粒度的门控机制（超越对角门）

2. **应用层面**：
   - 1M+ context 将成为高端模型标配
   - Agent/RL 场景广泛采用 Linear
   - 检索增强（RAG）与 Linear 深度整合

3. **硬件层面**：
   - 专门优化的 DPLR 芯片
   - 混合架构的异构加速
   - 更高效的 Chunking 实现

---

## 参考文献

1. Katharopoulos et al., "Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention", 2020
2. Choromanski et al., "Rethinking Attention with Performers", 2021
3. Yang et al., "Gated Linear Attention Transformers with Hardware-Efficient Training", 2023 (GLA)
4. Yang et al., "Parallelizing Linear Transformers with the Delta Rule over Sequence Length", 2024 (The mathematical basis for KDA)
5. Dao et al., "FlashAttention: Fast and Memory-Efficient Exact Attention", 2022
6. Kimi Team, "Kimi Linear Attention", 2025
7. Widrow & Hoff, "Adaptive Switching Circuits", 1960
8. Hopfield, "Neural networks and physical systems", 1982
9. Ramsauer et al., "Hopfield Networks is All You Need", 2020

---

## 附录

### 附录 A：Delta Rule 的详细推导

Delta Rule（也称 Widrow-Hoff 规则或 LMS 规则）是机器学习中最基础的学习规则之一，诞生于 1960 年。

#### A.1 单神经元的 Delta Rule

**问题设定**：

给定：
- 输入：x ∈ R^d
- 权重：w ∈ R^d
- 目标输出：y ∈ R
- 预测输出：ŷ = w^T x

**目标**：最小化平方误差

```
L(w) = 1/2 (y - ŷ)²
     = 1/2 (y - w^T x)²
```

**梯度下降**：

对 w 求梯度：

```
∇_w L = ∂/∂w [1/2 (y - w^T x)²]
      = -(y - w^T x) · x
      = -(y - ŷ) · x
```

梯度下降更新（学习率 η）：

```
w_new = w_old - η ∇_w L
      = w_old + η (y - ŷ) x
      = w_old + η · error · x
```

这就是 **Delta Rule**：

```
Δw = η · (target - prediction) · input
```

#### A.2 从标量到矩阵：DeltaNet 的扩展

**DeltaNet 问题设定**：

给定：
- 键：k_t ∈ R^d
- 值：v_t ∈ R^d
- 状态矩阵：S_t ∈ R^(d×d)
- 预测：v̂_t = S_t^T k_t

**目标**：最小化重构误差

```
L_t(S) = 1/2 |v_t - v̂_t|²
       = 1/2 |v_t - S_t^T k_t|²
```

**梯度下降**：

对 S 求梯度（注意这是矩阵对矩阵的导数）：

```
∇_S L = ∂/∂S [1/2 |v - S^T k|²]
```

使用链式法则和矩阵微分：

```
∂/∂S [|v - S^T k|²] = ∂/∂S [(v - S^T k)^T (v - S^T k)]
                     = ∂/∂S [v^T v - 2v^T S^T k + k^T S S^T k]
                     = -2 k v^T + 2 k k^T S
                     = 2 k (k^T S - v^T)
```

因此：

```
∇_S L = k (S^T k - v)^T
      = k (k^T S - v^T)
      = (S^T k - v) k^T
```

**更新规则**（学习率 β_t）：

```
S_t = S_{t-1} - β_t ∇_S L
    = S_{t-1} - β_t (S_{t-1}^T k_t - v_t) k_t^T
    = S_{t-1} - β_t k_t k_t^T S_{t-1} + β_t k_t v_t^T
    = (I - β_t k_t k_t^T) S_{t-1} + β_t k_t v_t^T
```

这就是 DeltaNet 的更新公式！

#### A.3 Delta Rule 在不同领域的体现

| 领域 | 形式 | 解释 |
|------|------|------|
| **感知机学习（1960）** | Δw = η(y - ŷ)x | 单神经元 |
| **LMS 滤波（信号处理）** | w(n+1) = w(n) + μe(n)x(n) | 自适应滤波 |
| **RLS（递归最小二乘）** | P(n+1) = P(n) - ... | 带遗忘因子 |
| **Rescorla-Wagner（神经科学）** | ΔV = αβ(λ - V) | 条件反射学习 |
| **Hopfield 网络（1982）** | w_ij = Σ x_i^μ x_j^μ | 联想记忆 |
| **DeltaNet（2023）** | ΔS = β(v - S^T k)k^T | Transformer 中的应用 |

#### A.4 为什么 Delta Rule 如此重要？

1. **生物学合理性**：
   - 误差驱动学习符合神经科学观察
   - Rescorla-Wagner 模型解释了经典条件反射
   - 神经突触的 Hebbian 学习的推广

2. **数学优雅性**：
   - 有明确的优化目标（最小化误差）
   - 梯度下降的自然结果
   - 可以严格证明收敛性

3. **工程实用性**：
   - 在线学习（无需批处理）
   - 增量更新（适合流式数据）
   - 计算简单（一次矩阵乘法）

4. **统一性**：
   - 连接了感知机、LMS、RLS、Hopfield
   - 从标量到矩阵的自然扩展
   - 从监督学习到无监督学习的桥梁

---

### 附录 B：DPLR 矩阵的封闭性证明

DPLR（Diagonal Plus Low-Rank）矩阵是形如以下的矩阵：

```
A = D + U V^T
```

其中：
- D ∈ R^(d×d) 是对角矩阵
- U ∈ R^(d×r)，V ∈ R^(d×r)（r << d）

**关键性质**：DPLR 矩阵的乘积仍然是 DPLR 矩阵（rank 可能增加但保持有界）。

#### B.1 简单情况：两个 DPLR 矩阵的乘积

**给定**：

```
A_1 = D_1 + U_1 V_1^T
A_2 = D_2 + U_2 V_2^T
```

**目标**：证明 A_1 A_2 也是 DPLR 形式。

**证明**：

```
A_1 A_2 = (D_1 + U_1 V_1^T)(D_2 + U_2 V_2^T)
        = D_1 D_2 + D_1 U_2 V_2^T + U_1 V_1^T D_2 + U_1 V_1^T U_2 V_2^T
```

第一项是对角矩阵：

```
D_1 D_2 = Diag(d_1 ⊙ d_2)
```

后三项都是低秩矩阵：

```
D_1 U_2 V_2^T       是 rank-r_2
U_1 V_1^T D_2       是 rank-r_1  
U_1 V_1^T U_2 V_2^T 是 rank-min(r_1, r_2)
```

关键技巧：**对角矩阵与低秩矩阵相乘仍是低秩**：

```
D_1 U_2 = [d_1[1]u_2[1], d_1[2]u_2[2], ..., d_1[d]u_2[d]]
        = Ũ_2  （仍然是 d×r_2 矩阵）

U_1 V_1^T D_2 = U_1 (D_2 V_1)^T
              = U_1 Ṽ_1^T
```

对于最后一项：

```
U_1 V_1^T U_2 V_2^T = U_1 (V_1^T U_2) V_2^T
                     = U_1 M V_2^T  （其中 M = V_1^T U_2 是 r_1×r_2 矩阵）
```

由于 M 很小，可以做 SVD 或 QR 分解保持低秩。

**合并所有低秩项**：

```
A_1 A_2 = D_1 D_2 + [D_1 U_2, U_1 Ṽ_1, U_1 M] [V_2, D_2 V_1, V_2]^T
        = D_3 + U_3 V_3^T
```

其中：
- D_3 = D_1 D_2（对角）
- U_3 = [D_1 U_2, U_1 Ṽ_1, U_1 M]
- V_3 = [V_2, D_2 V_1, V_2]
- rank = r_1 + r_2 + min(r_1, r_2) ≤ 3 max(r_1, r_2)

**结论**：DPLR 乘以 DPLR 仍是 DPLR，但 rank 可能增加。

#### B.2 DeltaNet 的特殊情况

在 DeltaNet 中，每步更新是：

```
A_t = D_t - β_t k_t k_t^T
```

这是 **Diagonal - Rank-1** 形式的 DPLR（符号为负也无妨）。

**关键观察**：

```
(D_1 - u_1 v_1^T)(D_2 - u_2 v_2^T)
= D_1 D_2 - D_1 u_2 v_2^T - u_1 v_1^T D_2 + u_1 (v_1^T u_2) v_2^T
= D_1 D_2 - [D_1 u_2, u_1] [v_2, (v_1^T u_2) v_2]^T
= D' - U' V'^T
```

其中：
- D' = D_1 D_2
- U' = [D_1 u_2, u_1]  （d×2 矩阵）
- V' = [v_2, (v_1^T u_2) v_2]  （d×2 矩阵）

**rank 增长分析**：

对于 n 步连乘：

```
A_n A_{n-1} ... A_1
```

如果每步都是 rank-1：
- 理论上 rank 可达 n
- **但实际上可以通过截断 SVD 控制 rank**
- 或使用 WY representation 稳定 rank

#### B.3 WY Representation：稳定 rank 的关键

WY representation 来自 Householder 反射的理论。

**定理**（Schur）：

对于一系列 Householder 反射：

```
Q = (I - τ_1 u_1 u_1^T)(I - τ_2 u_2 u_2^T)...(I - τ_r u_r u_r^T)
```

可以改写为：

```
Q = I - U T U^T
```

其中：
- U = [u_1, u_2, ..., u_r]  （d×r）
- T 是 r×r 的上三角矩阵

**递推构造 T**：

```
T_1 = τ_1
T_2 = [τ_1        -τ_1 τ_2 u_1^T u_2]
      [0          τ_2              ]
...
```

**应用到 DeltaNet**：

DeltaNet 的更新形如：

```
A_t = (I - β_t k_t k_t^T) D_t
```

虽然不是严格的 Householder 形式，但可以用类似技巧：

1. 收集所有 rank-1 项：U = [k_1, k_2, ..., k_n]
2. 构造交互矩阵 T（捕捉 k_i^T k_j 的交叉项）
3. 保持 D 作为对角部分

**最终形式**（chunk 内）：

```
A_chunk = D_chunk - U_chunk T_chunk V_chunk^T
```

其中：
- D_chunk：所有对角矩阵的乘积
- U_chunk, V_chunk：收集的 rank-1 项
- T_chunk：捕捉交叉项的小矩阵
- rank ≤ chunk_size（实践中远小于此）

#### B.4 为什么 rank 不会爆炸？

**理论原因**：

1. **数值截断**：
   - 小奇异值可以直接丢弃
   - 误差累积可控（Eckart-Young 定理）

2. **结构正则化**：
   - β_t 通常 < 1
   - k_t k_t^T 的主方向有限
   - 重复方向会被"合并"

3. **门控衰减**：
   - D_t = Diag(α_t) 中 α_t < 1
   - 旧的 rank-1 项会被指数衰减
   - 只有"活跃"方向贡献 rank

**实践观察**（来自 Kimi 论文）：

- chunk_size = 64 时，有效 rank ≈ 8-16
- chunk_size = 256 时，有效 rank ≈ 20-30
- 远小于理论上界（chunk_size）

#### B.5 计算复杂度分析

**标准矩阵乘法**：

```
S_t = A_t S_{t-1}
复杂度：O(d³)
```

**DPLR 矩阵乘法**：

```
S_t = (D - U V^T) S_{t-1}
    = D S_{t-1} - U (V^T S_{t-1})
    
步骤1：V^T S_{t-1}     → O(r d²)
步骤2：U × 结果          → O(r d²)
步骤3：D S_{t-1}        → O(d²)
总复杂度：O(r d²)
```

当 r << d 时，这是巨大的加速！

**Chunk-based 的优势**：

```
串行：n 次 O(d²) 更新 → O(n d²)
Chunk：n/C 次 O(r d²) 更新 → O(n r d²/C)
```

由于 r/C << 1 且可以并行，实际加速非常显著。

#### B.6 数值稳定性

**潜在问题**：

1. 对角矩阵连乘可能导致数值下溢/溢出
2. 低秩部分的累积可能损失精度

**解决方案**：

1. **对数空间计算对角部分**：
   ```
   log D_chunk = Σ log D_t
   ```

2. **正交化 U, V**：
   定期对 U, V 做 QR 分解保持正交性

3. **混合精度**：
   对角部分用 FP32，低秩部分用 FP16

4. **周期性重置**：
   每隔一定 tokens 从头开始累积

---

### 附录 C：与其他方法的数学对比

#### C.1 Linear Attention vs Softmax Attention

| 维度 | Softmax Attention | Linear Attention |
|------|------------------|------------------|
| **形式** | y = Σ softmax(q^T k) v | y = φ(q)^T S / φ(q)^T z |
| **非线性** | exp(·) | 特征映射 φ(·) |
| **归一化** | 显式（softmax） | 隐式（分母） |
| **复杂度** | O(n²) | O(n) |
| **并行性** | 天然并行 | 需要 scan 或 chunk |
| **记忆** | 无状态（每次重算） | 有状态（S 累积） |

#### C.2 DeltaNet vs Mamba (SSM)

| 维度 | DeltaNet/KDA | Mamba |
|------|-------------|-------|
| **状态空间** | S ∈ R^(d×d) | h ∈ R^(d×n) |
| **更新机制** | Delta Rule（误差驱动） | 状态空间模型（系统动力学） |
| **结构** | DPLR（封闭形式） | 一般矩阵（需要 scan） |
| **chunk 能力** | ✓ 可以 chunk | ✗ 难以 chunk |
| **理论渊源** | Hopfield, LMS | 控制论, Kalman 滤波 |

#### C.3 不同门控机制的数学表达

| 门控类型 | 数学形式 | 参数量 | 表达能力 |
|---------|---------|-------|---------|
| **无门控** | S_t = A S_{t-1} + B | 0 | ★☆☆☆☆ |
| **标量门** | S_t = α A S_{t-1} + B | 1 | ★★☆☆☆ |
| **对角门** | S_t = A Diag(α) S_{t-1} + B | d | ★★★★☆ |
| **全矩阵门** | S_t = A G S_{t-1} + B | d² | ★★★★★ |

**KDA 选择对角门的数学理由**：

1. **保持 DPLR 结构**：
   ```
   (D_1 - uk^T) Diag(α) 仍是 DPLR
   ```

2. **可分解计算**：
   ```
   Diag(α) S = [α_1 s_1, α_2 s_2, ..., α_d s_d]
   ```
   每列独立，完全并行

3. **等价于可学习位置编码**：
   ```
   α_t^(i) = exp(-decay_i · t)
   ```
   每个维度有独立的时间尺度

---

### 附录 D：实现细节与优化技巧

#### D.1 高效实现 DPLR 矩阵乘法

```python
def dplr_matmul(D, U, V, S):
    """
    计算 (D - U V^T) S
    
    Args:
        D: (d,) 对角元素
        U: (d, r) 低秩左矩阵
        V: (d, r) 低秩右矩阵
        S: (d, d) 或 (d, d_v) 状态矩阵
    
    Returns:
        result: (D - U V^T) S
    """
    # 步骤1：对角部分 O(d²) 或 O(d·d_v)
    DS = D[:, None] * S  # 广播乘法
    
    # 步骤2：低秩部分 O(r·d·d_v)
    VS = V.T @ S      # (r, d_v)
    UVS = U @ VS      # (d, d_v)
    
    # 步骤3：合并
    result = DS - UVS
    
    return result
```

**复杂度分析**：
- 传统：O(d³) 或 O(d²·d_v)
- DPLR：O(d·d_v) + O(r·d·d_v) = O((1+r)·d·d_v)

当 r << d 时，加速约 d/(1+r) 倍。

#### D.2 Chunk-based 并行化伪代码

```python
def chunk_based_linear_attention(K, V, chunk_size=64):
    """
    Chunk-based KDA with DPLR
    
    Args:
        K: (n, d) 键序列
        V: (n, d_v) 值序列
        chunk_size: chunk 大小
    
    Returns:
        Y: (n, d_v) 输出序列
    """
    n, d = K.shape
    n_chunks = (n + chunk_size - 1) // chunk_size
    
    # 初始化状态
    S = torch.zeros(d, d_v)
    outputs = []
    
    for chunk_id in range(n_chunks):
        start = chunk_id * chunk_size
        end = min(start + chunk_size, n)
        
        K_chunk = K[start:end]
        V_chunk = V[start:end]
        
        # 构造 chunk 内的 DPLR 矩阵
        D_chunk, U_chunk, V_chunk_lr, T_chunk = \
            build_chunk_dplr(K_chunk, V_chunk)
        
        # 并行计算 chunk 内所有输出
        Y_chunk = compute_chunk_output(
            K_chunk, S, D_chunk, U_chunk, V_chunk_lr, T_chunk
        )
        outputs.append(Y_chunk)
        
        # 更新状态（这一步是串行的）
        S = dplr_matmul(D_chunk, U_chunk, T_chunk @ V_chunk_lr.T, S)
        S = S + accumulate_chunk_updates(K_chunk, V_chunk)
    
    return torch.cat(outputs, dim=0)

def build_chunk_dplr(K_chunk, V_chunk):
    """
    构造 chunk 的 DPLR 表示
    使用 WY representation
    """
    C = K_chunk.shape[0]
    
    # 收集所有 rank-1 更新
    U = K_chunk  # (C, d)
    V = K_chunk  # (C, d)
    
    # 构造交互矩阵 T（上三角）
    T = torch.zeros(C, C)
    for i in range(C):
        for j in range(i+1):
            if i == j:
                T[i,j] = -beta[i]
            else:
                T[i,j] = -beta[i] * (V[i] @ U[j])
    
    # 收集对角门控
    D_chunk = compute_diagonal_gates(K_chunk)
    
    return D_chunk, U, V, T
```

#### D.3 数值稳定性技巧

```python
def stable_diagonal_product(D_list):
    """
    稳定计算多个对角矩阵的乘积
    使用对数空间避免下溢/溢出
    """
    log_D = sum(torch.log(D + 1e-8) for D in D_list)
    D_product = torch.exp(log_D)
    return D_product

def orthogonalize_uv(U, V):
    """
    定期正交化 U, V 以保持数值稳定
    """
    Q_u, R_u = torch.qr(U)
    Q_v, R_v = torch.qr(V)
    # 修正：V^T 的正交化
    return Q_u, Q_v @ R_v @ R_u.T
```

---