---
title: å¹¶è¡Œè®¡ç®—ä¸“é¢˜
---

# ğŸš€ å¹¶è¡Œè®¡ç®—ä¸“é¢˜

è®°å½•å¹¶è¡Œè®¡ç®—é¢†åŸŸçš„å­¦ä¹ ç¬”è®°å’Œå®è·µç»éªŒã€‚

## ğŸ“š æ–‡ç« åˆ—è¡¨

## æ˜‡è…¾NPUç¼–ç¨‹
- [ä»‹ç»ï¼šä»è‹±ä¼Ÿè¾¾å’Œæ˜‡è…¾ä¸åŒçš„åœ°æ–¹è®²èµ·ï¼ŒCPUä¸€ä¾§](./ascend/intro.md)
- [ä»‹ç»ï¼šç»§ç»­ä¸Šä¸€ç¯‡ï¼Œè®²è§£NPU Kernelå‡½æ•°](./ascend/intro_npu.md)
- [AclrtSynchronizeStreamå®æˆ˜ç¤ºä¾‹](./ascend/aclrtSynchStream_example.md)
## å¹¶è¡Œè®¡ç®—è¯¾ç¨‹
- [ç¬¬ä¸€è¯¾ï¼šå¦‚ä½•åŠ é€ŸçŸ©é˜µçš„ä¹˜æ³•ï¼Œå‰ç½®çŸ¥è¯†ã€ç¼“å­˜æœç”¨å’Œåˆ†å—](./lesson/ParallelC-lesson1)
- [ç¬¬ä¸‰è¯¾ï¼šå¤šæ ¸ç¼–ç¨‹-å‰ç½®çŸ¥è¯†](./lesson/ParallelC-lesson3_pre.md)
- [ç¬¬ä¸‰è¯¾ï¼šå¤šæ ¸ç¼–ç¨‹](./lesson/ParallelC-lesson3.md)


## çº¿æ€§æ³¨æ„åŠ›/Linear Attention
- [ä»ä¼ ç»Ÿæ³¨æ„åŠ›åˆ°çº¿æ€§æ³¨æ„åŠ›](./linear-attention/softmaxToLinear.md)
- [From Standard Attention to Linear Attention](./linear-attention/softmaxToLinear-en.md)
- [çº¿æ€§æ³¨æ„åŠ›çš„æ¼”åŒ–è¿‡ç¨‹](./linear-attention/evolution.md)
- [Evolution of Linear Attention](./linear-attention/evolution-english.md)
- [Performerä¸ºä»€ä¹ˆåœ¨æœŸæœ›ä¸Šèƒ½åšåˆ°å’ŒSoftmaxç›¸ç­‰](./linear-attention/performer.md)
- [Why Performer is equal to Softmax attention in the aspect of expectation](./linear-attention//performer-en.md)


## é—®ç­”
- [ä¸ºä»€ä¹ˆå•†ä¸šå…¬å¸é€‰æ‹©GPT,è€Œä¸æ˜¯BERT?](./ask&answer/bert-vs-gpt-commerical-performance.md)
- [æ˜¯å¦æœ‰å¿…è¦åœ¨è¿›è¡Œsoftmaxå‡å»æœ€å¤§å€¼ï¼Œä¸å‡çš„è¯æ˜¯å¦å½±å“ç²¾åº¦?](./ask&answer/is_minus_max_necessary.md)
- [åˆ©ç”¨å±€éƒ¨æ€§å’Œç°‡çŠ¶æœ€å¤§æ¥æŠ½æ ·Softmax](./ask&answer/sampling_softmax_by_locality.md)
- [æ˜¯å¦èƒ½ç®—å‡ºä¸€è¡Œä»¥åç›´æ¥è¿›è¡Œsoftmax?](./ask&answer/new.md)
