<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>FlashAttention on Ascend NPU ‚Äî Yunlong Zhou</title>
    <link href="https://fonts.googleapis.com/css2?family=Instrument+Serif:ital@0;1&family=DM+Sans:wght@300;400;500;600&family=JetBrains+Mono:wght@400;500&display=swap" rel="stylesheet">
    <style>
        :root {
            --ink: #0a0a0a;
            --paper: #faf9f7;
            --muted: #71717a;
            --border: #e4e4e7;
            --accent-green: #10b981;
            --accent-green-light: rgba(16, 185, 129, 0.1);
            --accent-red: #dc2626;
            --accent-blue: #06b6d4;
        }

        * { box-sizing: border-box; margin: 0; padding: 0; }
        html { scroll-behavior: smooth; }

        body {
            font-family: 'DM Sans', -apple-system, sans-serif;
            background: var(--paper);
            color: var(--ink);
            line-height: 1.7;
        }

        /* Navigation */
        .nav {
            position: fixed;
            top: 0;
            left: 0;
            right: 0;
            padding: 1.5rem 2rem;
            background: rgba(250, 249, 247, 0.9);
            backdrop-filter: blur(10px);
            border-bottom: 1px solid var(--border);
            z-index: 100;
            display: flex;
            justify-content: space-between;
            align-items: center;
        }

        .nav-back {
            font-family: 'JetBrains Mono', monospace;
            font-size: 0.75rem;
            text-transform: uppercase;
            letter-spacing: 0.1em;
            color: var(--muted);
            text-decoration: none;
            display: flex;
            align-items: center;
            gap: 0.5rem;
            transition: color 0.2s;
        }

        .nav-back:hover {
            color: var(--ink);
        }

        .nav-title {
            font-family: 'JetBrains Mono', monospace;
            font-size: 0.7rem;
            color: var(--accent-green);
        }

        /* Hero */
        .hero {
            padding: 10rem 2rem 6rem;
            max-width: 900px;
            margin: 0 auto;
        }

        .hero-label {
            font-family: 'JetBrains Mono', monospace;
            font-size: 0.65rem;
            text-transform: uppercase;
            letter-spacing: 0.2em;
            color: var(--accent-green);
            padding: 0.4rem 0.8rem;
            background: var(--accent-green-light);
            border-radius: 4px;
            display: inline-block;
            margin-bottom: 2rem;
        }

        .hero-title {
            font-family: 'Instrument Serif', Georgia, serif;
            font-size: clamp(2rem, 5vw, 3rem);
            font-weight: 400;
            line-height: 1.2;
            letter-spacing: -0.02em;
            margin-bottom: 1.5rem;
        }

        .hero-subtitle {
            font-size: 1.15rem;
            color: var(--muted);
            max-width: 650px;
            margin-bottom: 3rem;
        }

        .hero-meta {
            display: flex;
            flex-wrap: wrap;
            gap: 2rem;
            padding-top: 2rem;
            border-top: 1px solid var(--border);
        }

        .meta-item {
            display: flex;
            flex-direction: column;
            gap: 0.25rem;
        }

        .meta-label {
            font-family: 'JetBrains Mono', monospace;
            font-size: 0.65rem;
            text-transform: uppercase;
            letter-spacing: 0.1em;
            color: var(--muted);
        }

        .meta-value {
            font-size: 0.95rem;
            font-weight: 500;
        }

        /* Content Sections */
        .content {
            max-width: 900px;
            margin: 0 auto;
            padding: 0 2rem 6rem;
        }

        .section {
            margin-bottom: 5rem;
        }

        .section-label {
            font-family: 'JetBrains Mono', monospace;
            font-size: 0.65rem;
            text-transform: uppercase;
            letter-spacing: 0.2em;
            color: var(--accent-green);
            margin-bottom: 1rem;
        }

        .section-title {
            font-family: 'Instrument Serif', Georgia, serif;
            font-size: 1.75rem;
            font-weight: 400;
            margin-bottom: 1.5rem;
        }

        .section p {
            color: var(--ink);
            margin-bottom: 1.25rem;
        }

        .section p:last-child {
            margin-bottom: 0;
        }

        /* Highlight Box */
        .highlight-box {
            background: white;
            border: 1px solid var(--border);
            border-left: 4px solid var(--accent-green);
            border-radius: 8px;
            padding: 1.5rem 2rem;
            margin: 2rem 0;
        }

        .highlight-box p {
            font-family: 'Instrument Serif', Georgia, serif;
            font-style: italic;
            font-size: 1.05rem;
            color: var(--ink);
            margin: 0;
        }

        /* Role Card */
        .role-card {
            background: var(--ink);
            color: white;
            border-radius: 16px;
            padding: 2.5rem;
            margin: 3rem 0;
        }

        .role-card .section-label {
            color: rgba(255,255,255,0.4);
        }

        .role-card .section-title {
            color: white;
            margin-bottom: 1.5rem;
        }

        .role-list {
            display: flex;
            flex-direction: column;
            gap: 1rem;
        }

        .role-item {
            display: flex;
            gap: 1rem;
            align-items: flex-start;
        }

        .role-number {
            font-family: 'JetBrains Mono', monospace;
            font-size: 0.75rem;
            color: var(--accent-green);
            background: rgba(16, 185, 129, 0.15);
            padding: 0.25rem 0.5rem;
            border-radius: 4px;
            flex-shrink: 0;
        }

        .role-text {
            font-size: 0.95rem;
            color: rgba(255,255,255,0.85);
            line-height: 1.6;
        }

        /* Insight Grid */
        .insight-grid {
            display: grid;
            grid-template-columns: repeat(2, 1fr);
            gap: 1.5rem;
            margin: 2rem 0;
        }

        .insight-card {
            background: white;
            border: 1px solid var(--border);
            border-radius: 12px;
            padding: 1.75rem;
        }

        .insight-card.full-width {
            grid-column: 1 / -1;
        }

        .insight-icon {
            font-size: 1.5rem;
            margin-bottom: 1rem;
        }

        .insight-title {
            font-weight: 600;
            font-size: 1rem;
            margin-bottom: 0.5rem;
        }

        .insight-desc {
            font-size: 0.9rem;
            color: var(--muted);
            line-height: 1.6;
        }

        /* Stats Row */
        .stats-row {
            display: flex;
            gap: 3rem;
            padding: 2rem 0;
            border-top: 1px solid var(--border);
            border-bottom: 1px solid var(--border);
            margin: 2rem 0;
        }

        .stat {
            display: flex;
            flex-direction: column;
        }

        .stat-number {
            font-family: 'JetBrains Mono', monospace;
            font-size: 2.5rem;
            font-weight: 600;
            color: var(--accent-green);
            line-height: 1;
        }

        .stat-label {
            font-size: 0.85rem;
            color: var(--muted);
            margin-top: 0.5rem;
        }

        /* Question Cards */
        .question-section {
            background: white;
            border: 1px solid var(--border);
            border-radius: 16px;
            padding: 2.5rem;
            margin: 3rem 0;
        }

        .question-list {
            display: flex;
            flex-direction: column;
            gap: 1.5rem;
        }

        .question-item {
            padding-left: 1.5rem;
            border-left: 2px solid var(--accent-blue);
        }

        .question-text {
            font-family: 'Instrument Serif', Georgia, serif;
            font-size: 1.1rem;
            font-style: italic;
            color: var(--ink);
            margin-bottom: 0.5rem;
        }

        .question-context {
            font-size: 0.9rem;
            color: var(--muted);
        }

        /* Timeline */
        .timeline {
            position: relative;
            padding-left: 2rem;
            margin: 2rem 0;
        }

        .timeline::before {
            content: '';
            position: absolute;
            left: 0;
            top: 0;
            bottom: 0;
            width: 2px;
            background: var(--border);
        }

        .timeline-item {
            position: relative;
            padding-bottom: 2rem;
        }

        .timeline-item:last-child {
            padding-bottom: 0;
        }

        .timeline-item::before {
            content: '';
            position: absolute;
            left: -2rem;
            top: 0.5rem;
            width: 10px;
            height: 10px;
            background: var(--accent-green);
            border-radius: 50%;
            transform: translateX(-4px);
        }

        .timeline-phase {
            font-family: 'JetBrains Mono', monospace;
            font-size: 0.7rem;
            text-transform: uppercase;
            letter-spacing: 0.1em;
            color: var(--accent-green);
            margin-bottom: 0.5rem;
        }

        .timeline-title {
            font-weight: 600;
            margin-bottom: 0.5rem;
        }

        .timeline-desc {
            font-size: 0.9rem;
            color: var(--muted);
        }

        /* Footer */
        footer {
            padding: 4rem 2rem;
            background: white;
            border-top: 1px solid var(--border);
        }

        .footer-inner {
            max-width: 900px;
            margin: 0 auto;
            display: flex;
            justify-content: space-between;
            align-items: center;
        }

        .footer-nav {
            display: flex;
            gap: 2rem;
        }

        .footer-link {
            font-size: 0.9rem;
            color: var(--muted);
            text-decoration: none;
            transition: color 0.2s;
        }

        .footer-link:hover {
            color: var(--ink);
        }

        .footer-contact {
            font-size: 0.85rem;
            color: var(--muted);
        }

        @media (max-width: 768px) {
            .hero-meta {
                flex-direction: column;
                gap: 1rem;
            }

            .insight-grid {
                grid-template-columns: 1fr;
            }

            .stats-row {
                flex-direction: column;
                gap: 1.5rem;
            }

            .footer-inner {
                flex-direction: column;
                gap: 1.5rem;
                text-align: center;
            }
        }
    </style>
</head>
<body>

<nav class="nav">
    <a href="index.html" class="nav-back">‚Üê Back to Portfolio</a>
    <span class="nav-title">Systems Optimization</span>
</nav>

<section class="hero">
    <span class="hero-label">Systems Optimization</span>
    <h1 class="hero-title">FlashAttention on Ascend NPU: Diagnosing the Real Bottleneck</h1>
    <p class="hero-subtitle">A first-principles investigation that revealed why GPU-optimized attention algorithms fail on different architectures‚Äîand what this teaches us about hardware-software co-design.</p>
    
    <div class="hero-meta">
        <div class="meta-item">
            <span class="meta-label">Context</span>
            <span class="meta-value">Fudan Parallel Computing Group</span>
        </div>
        <div class="meta-item">
            <span class="meta-label">Duration</span>
            <span class="meta-value">Fall 2025</span>
        </div>
        <div class="meta-item">
            <span class="meta-label">Outcome</span>
            <span class="meta-value">~10% End-to-End Speedup</span>
        </div>
    </div>
</section>

<div class="content">

    <!-- The Problem -->
    <section class="section">
        <div class="section-label">The Problem</div>
        <h2 class="section-title">Why was FlashAttention slow on Ascend?</h2>
        <p>FlashAttention is the gold standard for efficient attention computation on NVIDIA GPUs. When our team attempted to port it to Ascend NPU, we observed severe performance degradation. The initial diagnosis blamed the softmax computation‚Äîspecifically, the overhead of repeatedly finding row-wise maxima for numerical stability.</p>
        <p>This seemed plausible. Online Softmax requires streaming through attention scores to track running maxima, and the team assumed this O(n) scan was the culprit.</p>
        
        <div class="highlight-box">
            <p>The team had a reasonable hypothesis. But reasonable hypotheses can be wrong.</p>
        </div>
    </section>

    <!-- My Role -->
    <div class="role-card">
        <div class="section-label">My Role</div>
        <h2 class="section-title">Three contributions to the diagnosis</h2>
        <div class="role-list">
            <div class="role-item">
                <span class="role-number">01</span>
                <span class="role-text"><strong>Challenged the numerical stability assumption.</strong> I proved mathematically that under conditions where overflow does not occur, the max-subtraction step in softmax can be skipped without precision loss. The conventional belief that "subtracting max improves precision" is a misconception‚Äîthe actual benefit is solely overflow prevention.</span>
            </div>
            <div class="role-item">
                <span class="role-number">02</span>
                <span class="role-text"><strong>Investigated semantic structure in attention patterns.</strong> I analyzed whether attention scores in pretrained models exhibit "clustered maxima"‚Äîwhere high values form contiguous regions rather than isolated spikes. They do. This linguistic structure could theoretically enable sampling-based max-finding. But even this optimization barely moved the needle.</span>
            </div>
            <div class="role-item">
                <span class="role-number">03</span>
                <span class="role-text"><strong>Identified the real bottleneck.</strong> When I bypassed the entire softmax calculation and saw almost no speedup, I knew the bottleneck had been misdiagnosed. Architectural analysis revealed the true culprit: synchronization overhead between Cube (matrix) and Vector (element-wise) processing units.</span>
            </div>
        </div>
    </div>

    <!-- Key Insight -->
    <section class="section">
        <div class="section-label">Key Insight</div>
        <h2 class="section-title">The architectural mismatch</h2>
        <p>NVIDIA GPUs facilitate high-throughput computation through a tightly coupled architecture where arithmetic units share access to fast on-chip memory and register files. Online Softmax leverages this by maintaining intermediate results within registers, requiring negligible communication overhead.</p>
        <p>Ascend NPUs are fundamentally different. The architecture employs a strict separation of concerns:</p>
        
        <div class="insight-grid">
            <div class="insight-card">
                <div class="insight-icon">üßä</div>
                <div class="insight-title">Cube Unit</div>
                <div class="insight-desc">Dedicated to high-throughput matrix multiplication (GEMM). Optimized for dense linear algebra.</div>
            </div>
            <div class="insight-card">
                <div class="insight-icon">üìê</div>
                <div class="insight-title">Vector Unit</div>
                <div class="insight-desc">Dedicated to non-linear operations (Softmax, LayerNorm). Element-wise computations.</div>
            </div>
            <div class="insight-card full-width">
                <div class="insight-icon">üîÑ</div>
                <div class="insight-title">The Critical Constraint: L2 Buffer Communication</div>
                <div class="insight-desc">Unlike GPUs, there is no shared register file between Cube and Vector units. Every intermediate result must be flushed to L2 cache before being consumed by the other unit. Each Cube-Vec transition is a hard synchronization barrier that cannot be overlapped or hidden through pipelining.</div>
            </div>
        </div>

        <p>The original implementation, ported with GPU-style streaming assumptions, triggered these synchronizations excessively. Small-block tiling that works beautifully on GPUs becomes a performance disaster when every block boundary forces an L2 round-trip.</p>
    </section>

    <!-- The Solution -->
    <section class="section">
        <div class="section-label">The Solution</div>
        <h2 class="section-title">Coarse-grained tiling strategy</h2>
        <p>My theoretical analysis showed that restructuring the tile strategy‚Äîcomputing entire rows before synchronizing rather than streaming small blocks‚Äîcould dramatically reduce L2 access operations.</p>
        
        <div class="stats-row">
            <div class="stat">
                <span class="stat-number">56%</span>
                <span class="stat-label">Reduction in L2 cache access operations</span>
            </div>
            <div class="stat">
                <span class="stat-number">~10%</span>
                <span class="stat-label">End-to-end speedup (validated)</span>
            </div>
            <div class="stat">
                <span class="stat-number">2‚Üí32</span>
                <span class="stat-label">Sync events reduced (per attention row)</span>
            </div>
        </div>

        <p>Based on my tiling strategy specification, a teammate implemented the restructuring. The speedup validated the diagnosis: the bottleneck was never softmax computation‚Äîit was the communication pattern between processing units.</p>
        
        <div class="highlight-box">
            <p>This insight generalizes: when I later analyzed linear attention variants for the same hardware, I found identical bottlenecks‚ÄîL2 communication dominating runtime regardless of the attention mechanism's theoretical complexity.</p>
        </div>
    </section>

    <!-- What's Not Solved -->
    <section class="section">
        <div class="section-label">What's Not Solved</div>
        <h2 class="section-title">The limits of diagnosis</h2>
        
        <div class="insight-grid">
            <div class="insight-card">
                <div class="insight-title">Implementation Gap</div>
                <div class="insight-desc">I provided the theoretical analysis and tiling specification, but the actual kernel implementation was done by a teammate. I can diagnose why systems underperform, but I haven't yet developed the low-level programming skills to build optimized kernels from scratch.</div>
            </div>
            <div class="insight-card">
                <div class="insight-title">Single-Device Scope</div>
                <div class="insight-desc">This work focused on single-accelerator optimization. Distributed training and inference introduce entirely new bottlenecks‚Äîcommunication patterns, consistency models, fault tolerance‚Äîthat I haven't studied systematically.</div>
            </div>
            <div class="insight-card">
                <div class="insight-title">Clustered Maxima: Unexploited</div>
                <div class="insight-desc">My analysis of attention score patterns showed promising structure for sampling-based optimization. But given that softmax wasn't the real bottleneck, I never fully developed this into a practical algorithm.</div>
            </div>
            <div class="insight-card">
                <div class="insight-title">Hardware Access Constraints</div>
                <div class="insight-desc">Working with Ascend NPUs requires navigating a proprietary ecosystem with limited documentation. Some architectural details remain opaque, limiting how far theoretical analysis can go.</div>
            </div>
        </div>
    </section>

    <!-- Research Journey -->
    <section class="section">
        <div class="section-label">Research Journey</div>
        <h2 class="section-title">How the investigation unfolded</h2>
        
        <div class="timeline">
            <div class="timeline-item">
                <div class="timeline-phase">Phase 1: Questioning Assumptions</div>
                <div class="timeline-title">Proved max-subtraction doesn't improve precision</div>
                <div class="timeline-desc">Mathematical analysis showed that both methods‚Äîwith and without max subtraction‚Äîproduce identical first-order rounding error of O(Œº). The "precision improvement" rationale is a widespread misconception.</div>
            </div>
            <div class="timeline-item">
                <div class="timeline-phase">Phase 2: Exploring Semantic Structure</div>
                <div class="timeline-title">Discovered clustered maxima in attention patterns</div>
                <div class="timeline-desc">Analyzed GPT-2 attention patterns across all 12 layers. Found that 50-70% of attention rows have neighbors within ¬±2 positions reaching 80% of the maximum value‚Äîmaxima cluster rather than spike.</div>
            </div>
            <div class="timeline-item">
                <div class="timeline-phase">Phase 3: The Pivot</div>
                <div class="timeline-title">Bypassed softmax, saw no speedup</div>
                <div class="timeline-desc">This was the critical experiment. If softmax were the bottleneck, removing it should help. It didn't. The bottleneck was elsewhere.</div>
            </div>
            <div class="timeline-item">
                <div class="timeline-phase">Phase 4: Architectural Analysis</div>
                <div class="timeline-title">Identified Cube-Vec synchronization as the real culprit</div>
                <div class="timeline-desc">Deep dive into Ascend architecture revealed the L2 buffer communication constraint. Modeled latency costs and derived optimal tiling strategy.</div>
            </div>
        </div>
    </section>

    <!-- Next Questions -->
    <div class="question-section">
        <div class="section-label">Next Questions</div>
        <h2 class="section-title">Where this leads</h2>
        
        <div class="question-list">
            <div class="question-item">
                <div class="question-text">How do we generalize hardware-aware co-design principles across different accelerator architectures?</div>
                <div class="question-context">Each new AI chip (TPU, Groq, Cerebras, custom ASICs) has unique memory hierarchies and compute patterns. Is there a systematic framework for algorithm-hardware matching?</div>
            </div>
            <div class="question-item">
                <div class="question-text">Can the clustered maxima property be exploited for other attention optimizations?</div>
                <div class="question-context">The semantic structure I discovered in attention patterns might enable sparse attention, approximate nearest neighbor search, or other efficiency techniques‚Äîeven if it didn't help with Ascend's specific bottleneck.</div>
            </div>
            <div class="question-item">
                <div class="question-text">What happens when we scale beyond single devices?</div>
                <div class="question-context">Distributed training introduces communication as a first-order constraint. The same principle‚Äîunderstanding where time actually goes‚Äîshould apply, but the analysis becomes more complex.</div>
            </div>
        </div>
    </div>

    <!-- Takeaway -->
    <section class="section">
        <div class="section-label">Takeaway</div>
        <h2 class="section-title">The meta-lesson</h2>
        <p>"Optimality" in deep learning systems is not an intrinsic algorithmic property‚Äîit's a function of the hardware-software interface. FlashAttention's Online Softmax is optimal for register-rich, shared-memory architectures like GPUs. It's suboptimal for the decoupled, L2-centric architecture of Ascend NPUs.</p>
        <p>This experience made me effective at one thing: <strong>diagnosis</strong>‚Äîunderstanding why a system underperforms. What I now seek is the complementary skill of <strong>synthesis</strong>‚Äîthe ability to architect and implement solutions from the ground up.</p>
        
        <div class="highlight-box">
            <p>The question that drove this work: "Why is this slow?" The question I want to answer next: "How do I make it fast?"</p>
        </div>
    </section>

</div>

<footer>
    <div class="footer-inner">
        <div class="footer-nav">
            <a href="index.html" class="footer-link">‚Üê Portfolio</a>
            <a href="mindforge.html" class="footer-link">MindForge ‚Üí</a>
        </div>
        <div class="footer-contact">ylzhou21@m.fudan.edu.cn</div>
    </div>
</footer>

</body>
</html>