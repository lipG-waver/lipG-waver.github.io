### INTRODUCTION
**Two Approaches to Problems:**
* Option 1: Understand the problem and design a solution (traditional approach)
* Option 2: Set it up as a machine learning problem using data and supervised/reinforcement learning

## 例子1：机器人抓取物体

### **Option 1: 传统设计方法**
```
工程师的思路：
1. 分析物理学：计算力学、摩擦系数、重心
2. 手工设计规则：
   - IF 物体是圆形 THEN 用三指夹持
   - IF 物体重量>500g THEN 增加握力
   - IF 物体在边缘 THEN 先推到中心
3. 编写控制算法：PID控制器调节力度
4. 针对每种物体类型写不同代码
```

**问题：** 遇到训练时没见过的新物体（比如软的、不规则的）就失败

### **Option 2: 机器学习方法**
```
数据驱动的思路：
1. 收集数据：让机器人尝试抓取1000次
   - 记录：(图像, 动作) → 成功/失败
2. 训练神经网络：
   输入：摄像头看到的物体图像
   输出：应该用什么抓取姿态
3. 机器人自己学会：
   "这种形状用这个角度成功率高"
```

**优势：** 能泛化到新物体，因为学到了抓取的"模式"而非规则

---

## 例子2：自动驾驶转弯

### **Option 1: 传统方法**
```python
# 工程师手写的规则
def turn_steering(lane_position, curve_radius):
    if lane_position > center_line + 0.5:
        steering_angle = -15  # 向左转
    elif curve_radius < 50:
        steering_angle = 30   # 急转弯
    else:
        steering_angle = calculate_by_formula(...)
    return steering_angle
```

需要为每种情况（雨天、夜晚、不同路面）写不同规则

### **Option 2: 机器学习方法**
```python
# 从数据学习（课程第46页提到的例子）
# 数据：人类驾驶员的示范
thousands_of_hours = [
    (camera_image_1, steering_angle_1),
    (camera_image_2, steering_angle_2),
    ...
]

# 训练神经网络
model.learn(input=images, output=steering_angles)

# 新情况下自动预测
new_steering = model.predict(current_camera_image)
```

**关键差异：** 不需要明确告诉系统"转弯角度=f(车道偏移, 曲率)"，它从数据中自己发现这种关系

---

## 例子3：游戏AI

### **Option 1: 传统游戏AI**
```
打砖块游戏的手写规则：
1. 计算球的轨迹方程
2. IF 球往左飞 THEN 挡板左移
3. IF 球速度>阈值 THEN 提前预判
4. 特殊情况：球卡在角落 → 执行特定动作序列
```

### **Option 2: 强化学习**
```
给AI的唯一信息：
- 观察：屏幕像素
- 动作：左移/右移/不动
- 奖励：+1（击中砖块）/ -1（漏球）

过程：
试验1: 随机动作 → 得分0
试验1000: 开始接到球 → 得分50  
试验100000: 发现"隧道效应"技巧 → 得分5000
         （这是人类玩家都不知道的策略！）
```

**RL能发现人类没想到的解决方案（"Move 37"）**

---

## 例子4：垃圾分类（文档第29页的真实应用）

### **Option 1: 传统视觉系统**
```
规则设计：
IF 物体颜色=蓝色 AND 形状=圆柱 THEN 可回收
IF 检测到"Coca-Cola"文字 THEN 塑料瓶类
需要为每种垃圾写判断逻辑
```

### **Option 2: 深度学习+强化学习**
```
数据收集：
- 拍摄10万张垃圾照片，人工标注类别
- 机器人尝试抓取，记录成功/失败

学习过程：
1. 图像识别网络学习：这是什么垃圾
2. 强化学习策略学习：怎么抓取不同材质
3. 结合决策：看到→识别→选择抓取方式→分类
```

**实际效果：** 能处理从未见过的新包装、变形的垃圾

---

## 核心理解

### **Option 1的思维模式**
```
人类专家 → 理解问题 → 设计算法 → 编程实现
        ↓
     领域知识
     物理公式  
     工程经验
```

### **Option 2的思维模式**
```
定义任务 → 收集数据 → 训练模型 → 自动发现规律
        ↓
     不需要理解"为什么"
     只需要大量示例
     机器自己学会"怎么做"
```

---

## 关键观点

**为什么要用ML方法？**

> "Impressive because **no person had thought of it**!"  
> (令人印象深刻，因为没有人想到这个解决方案)

vs.

> "Impressive because it **looks like something a person might draw**!"  
> (令人印象深刻，因为看起来像人会画的)

**这说明：**
- 传统方法（Option 1）：被人类思维限制
- ML方法（Option 2）：能超越人类直觉找到更优解

---

## 实际选择建议

| 场景 | 推荐方法 | 原因 |
|------|---------|------|
| 问题明确、规则简单 | Option 1 | 可解释、可靠 |
| 规则复杂、难以总结 | Option 2 | 人写不出完整规则 |
| 需要适应新情况 | Option 2 | 泛化能力强 |
| 安全关键系统 | 混合方法 | ML负责感知，规则保证安全 |

**本人的立场**  
未来趋势是Option 2，因为现实世界太复杂，人类无法为所有情况设计规则。

### 模仿学习和强化学习的区别

## 模仿学习 vs 强化学习

### 这节课讲的：模仿学习（Imitation Learning）
- **学习方式**：监督学习
- **需要什么**：专家演示数据 (观察, 动作) 对
- **目标**：学会模仿专家的行为
- **不需要**：奖励函数、环境交互试错
- **本质**：就是一个监督学习问题，只是应用在序贯决策上

```
专家数据：(o₁, a₁), (o₂, a₂), ..., (oₙ, aₙ)
        ↓
    监督学习训练
        ↓
    策略 π_θ(a|o)
```

### 强化学习（后续课程内容）
- **学习方式**：试错学习
- **需要什么**：环境、奖励函数
- **目标**：最大化累积奖励
- **不需要**：专家演示
- **本质**：智能体自己探索环境，通过奖励信号学习

```
随机策略 → 与环境交互 → 获得奖励 → 更新策略 → 重复
```

1. **模仿学习**：
   - 最简单的让智能体学习行为的方法
   - 但有很多局限性

2. **后续**：
   - 会讲真正的强化学习
   - 如何让智能体自己探索学习
   - 如何设计和使用奖励函数

3. **两者联系**：
   - 模仿学习相当于优化一个特殊的隐含奖励
   - `r(s,a) = log p(a = π*(s)|s)`
   - 这为理解强化学习做铺垫

## 为什么先讲模仿学习？

1. **概念简单**：就是监督学习，容易理解
2. **引出问题**：分布偏移 → 需要智能体自己探索
3. **实用价值**：在有专家数据时很有效
4. **平滑过渡**：从监督学习过渡到强化学习

## 简单类比

- **模仿学习** = 看着老师开车，学习模仿老师的操作
- **强化学习** = 自己开车练习，撞了扣分，开好了加分，慢慢学会

